{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# wikipedia specific info\n",
    "\n",
    "import wikipedia\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "# wikipedia categories to discard\n",
    "discard_categories = ['See also', 'References', 'External links', 'Further reading', \"Footnotes\",\n",
    "    \"Bibliography\", \"Sources\", \"Citations\", \"Literature\", \"Footnotes\", \"Notes and references\",\n",
    "    \"Photo gallery\", \"Works cited\", \"Photos\", \"Gallery\", \"Notes\", \"References and sources\",\n",
    "    \"References and notes\"]\n",
    "\n",
    "# Get the wikipedia page given a title\n",
    "def get_wiki_page(title):    \n",
    "    try:\n",
    "        return wikipedia.page(title)\n",
    "    except wikipedia.exceptions.DisambiguationError as e:\n",
    "        return wikipedia.page(e.options[0])\n",
    "    except wikipedia.exceptions.PageError as e:\n",
    "        raise Exception(\"Page not found\")\n",
    "\n",
    "# extract the sections of a wikipedia page into a df with title, heading and content\n",
    "# you can use a function similar to this for any website\n",
    "def extract_page(title: str):\n",
    "    wiki_text = get_wiki_page(title).content\n",
    "    content = re.split(r'==+ .* ==+', wiki_text)\n",
    "    headings = re.findall('==+ .* ==+', wiki_text)\n",
    "    # first element of content is the summary\n",
    "    df = pd.DataFrame({\n",
    "        'title': title,\n",
    "        'heading': \"summary\",\n",
    "        'content': content.pop(0).replace(\"\\n\", \" \").strip()\n",
    "    }, index=[0])\n",
    "\n",
    "    # add the rest of the sections\n",
    "    # len of content is equal len of headings as we popped the first element of content\n",
    "    for heading, cont in zip(headings, content):\n",
    "        plain_heading = \" \".join(heading.split(\" \")[1:-1]).strip()\n",
    "        # discarding the references and other low information sections\n",
    "        if plain_heading in discard_categories:\n",
    "            continue\n",
    "        cont = cont.replace(\"\\n\", \" \").strip()\n",
    "        if cont == \"\":\n",
    "            continue\n",
    "        df1 = pd.DataFrame({\n",
    "            'title': title,\n",
    "            'heading': plain_heading,\n",
    "            'content': cont\n",
    "        }, index=[0])\n",
    "        # concat the new section to the dataframe\n",
    "        df = pd.concat([df, df1], ignore_index=True)\n",
    "    return df\n",
    "\n",
    "# extract info for all pages in a list of titles and return a dataframe\n",
    "def wiki_extract(\n",
    "    titles: [str],\n",
    ") -> pd.DataFrame:\n",
    "    df = pd.DataFrame(columns=['title', 'heading', 'content'])\n",
    "    for title in titles: df = pd.concat([df, extract_page(title)], ignore_index=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create embeddings for given list of wikipedia pages, am not recursively adding pages since most likely the user will only give a few pages that are beyond the knowledge cutoff date\n",
    "# this is a very simple way to get embeddings, but it works for now\n",
    "\n",
    "# input = [\"Ingenuity (helicopter)\", \"List of Ingenuity flights\"]\n",
    "# print('upper bound cost estimate', [sum([cost_estimate(token_estimate(wiki.content)) for wiki in wiki_pages])])\n",
    "# df = wiki_extract(input)\n",
    "# df = get_df_embeddings(df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6 | packaged by conda-forge | (main, Aug 22 2022, 20:38:29) [Clang 13.0.1 ]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bdf0ee591c73da67f21f68f57a2216acdfa3ab25f9e605c52b032cc59455ae7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
